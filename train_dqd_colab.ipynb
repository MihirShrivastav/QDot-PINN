{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN for Double Quantum Dot Eigenstates (GaAs)\n",
    "\n",
    "**Self-contained notebook** - all code embedded, no external dependencies.\n",
    "\n",
    "Trains Physics-Informed Neural Networks to solve the single-electron Schrödinger eigenproblem for a 2D double quantum dot in GaAs.\n",
    "\n",
    "**Workflow:**\n",
    "1. Imports and constants\n",
    "2. Define physics classes (GaAs, BiquadraticParams)\n",
    "3. Define SIREN neural network\n",
    "4. Define loss functions\n",
    "5. Define training loop and helpers\n",
    "6. Configure parameters\n",
    "7. Train GS, ES1, ES2\n",
    "8. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Physical constants (SI)\n",
    "HBAR = 1.054_571_817e-34  # J*s\n",
    "E_CHARGE = 1.602_176_634e-19  # C\n",
    "EPS0 = 8.854_187_8128e-12  # F/m\n",
    "M0 = 9.109_383_7015e-31  # kg\n",
    "MEV_IN_J = 1.0e-3 * E_CHARGE\n",
    "\n",
    "print('✓ Imports successful')\n",
    "print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Physics Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class GaAs:\n",
    "    m_star_rel: float = 0.067\n",
    "    eps_r: float = 12.9\n",
    "    L0_nm: float = 30.0\n",
    "    \n",
    "    @property\n",
    "    def m_star(self): return self.m_star_rel * M0\n",
    "    @property\n",
    "    def L0(self): return self.L0_nm * 1e-9\n",
    "    @property\n",
    "    def E0_J(self): return (HBAR**2) / (2.0 * self.m_star * self.L0**2)\n",
    "    @property\n",
    "    def E0_meV(self): return self.E0_J / MEV_IN_J\n",
    "\n",
    "def kappa_from_hbar_omega_meV(hbar_omega_meV: float, mat: GaAs) -> float:\n",
    "    Eh_J = hbar_omega_meV * MEV_IN_J\n",
    "    return (mat.m_star**2 * mat.L0**4 / HBAR**4) * Eh_J**2\n",
    "\n",
    "@dataclass\n",
    "class BiquadraticParams:\n",
    "    a: float\n",
    "    c4: float\n",
    "    c2y: float\n",
    "    delta: float = 0.0\n",
    "\n",
    "def from_targets(a, hbar_omega_x_meV, hbar_omega_y_meV, delta=0.0, mat=None):\n",
    "    mat = mat or GaAs()\n",
    "    kx = kappa_from_hbar_omega_meV(hbar_omega_x_meV, mat)\n",
    "    ky = kappa_from_hbar_omega_meV(hbar_omega_y_meV, mat)\n",
    "    return BiquadraticParams(a=a, c4=kx/(4*a*a), c2y=ky, delta=delta)\n",
    "\n",
    "def vectorized_biq_v(p: BiquadraticParams, xy: torch.Tensor):\n",
    "    x, y = xy[:, 0:1], xy[:, 1:2]\n",
    "    return p.c4 * (x*x - p.a*p.a)**2 + p.c2y * y*y + p.delta * x\n",
    "\n",
    "print('✓ Physics classes defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SIREN Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=30.0):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "    def forward(self, x): return torch.sin(self.w0 * x)\n",
    "\n",
    "class SIRENLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, w0, is_first):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_features, out_features)\n",
    "        self.act = Sine(w0)\n",
    "        self.is_first, self.w0 = is_first, w0\n",
    "        with torch.no_grad():\n",
    "            if is_first:\n",
    "                self.lin.weight.uniform_(-1.0/in_features, 1.0/in_features)\n",
    "            else:\n",
    "                b = math.sqrt(6/in_features)/w0\n",
    "                self.lin.weight.uniform_(-b, b)\n",
    "            if self.lin.bias is not None: self.lin.bias.zero_()\n",
    "    def forward(self, x): return self.act(self.lin(x))\n",
    "\n",
    "class SIREN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=128, hidden_layers=4, out_features=1, w0=30.0, w0_hidden=1.0):\n",
    "        super().__init__()\n",
    "        layers = [SIRENLayer(in_features, hidden_features, w0, True)]\n",
    "        for _ in range(hidden_layers-1):\n",
    "            layers.append(SIRENLayer(hidden_features, hidden_features, w0_hidden, False))\n",
    "        self.net = nn.Sequential(*layers, nn.Linear(hidden_features, out_features))\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "print('✓ SIREN model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(fn, xy):\n",
    "    with torch.enable_grad():\n",
    "        xy = xy.detach().requires_grad_(True)\n",
    "        psi = fn(xy)\n",
    "        g = torch.autograd.grad(psi, xy, torch.ones_like(psi), create_graph=True, retain_graph=True)[0]\n",
    "        dx, dy = g[:, 0:1], g[:, 1:2]\n",
    "        dxx = torch.autograd.grad(dx, xy, torch.ones_like(dx), create_graph=True, retain_graph=True)[0][:, 0:1]\n",
    "        dyy = torch.autograd.grad(dy, xy, torch.ones_like(dy), create_graph=True, retain_graph=True)[0][:, 1:2]\n",
    "        return dxx + dyy\n",
    "\n",
    "def h_psi(fn, vfun, xy):\n",
    "    with torch.enable_grad():\n",
    "        return -laplacian(fn, xy) + vfun(xy) * fn(xy)\n",
    "\n",
    "def rayleigh_ritz_energy(fn, vfun, xy_q, w_q):\n",
    "    with torch.enable_grad():\n",
    "        xy = xy_q.detach().requires_grad_(True)\n",
    "        psi = fn(xy)\n",
    "        g = torch.autograd.grad(psi, xy, torch.ones_like(psi), create_graph=True, retain_graph=True)[0]\n",
    "        gsq = (g*g).sum(dim=1, keepdim=True)\n",
    "        v = vfun(xy)\n",
    "        num = (w_q * (gsq + v*psi*psi)).sum()\n",
    "        den = (w_q * psi*psi).sum()\n",
    "        return num / (den + 1e-12)\n",
    "\n",
    "def pde_residual_loss(fn, vfun, xy_c):\n",
    "    psi = fn(xy_c)\n",
    "    Hpsi = h_psi(fn, vfun, xy_c)\n",
    "    e_local = (psi*Hpsi).sum(dim=0, keepdim=True) / ((psi*psi).sum(dim=0, keepdim=True) + 1e-12)\n",
    "    res = Hpsi - e_local*psi\n",
    "    return (res*res).mean()\n",
    "\n",
    "def normalization_penalty(model, xy_q, w_q):\n",
    "    psi = model(xy_q)\n",
    "    return ((w_q*psi*psi).sum() - 1.0)**2\n",
    "\n",
    "def orthogonality_loss(psi_new, psi_prev, w_q):\n",
    "    num = (w_q*psi_new*psi_prev).sum()\n",
    "    den = torch.sqrt((w_q*psi_new*psi_new).sum() * (w_q*psi_prev*psi_prev).sum() + 1e-12)\n",
    "    return (num/(den+1e-12))**2\n",
    "\n",
    "def symmetry_penalty_odd(fn, xy_q, w_q):\n",
    "    with torch.enable_grad():\n",
    "        xy_flip = xy_q.clone()\n",
    "        xy_flip[:, 0:1] = -xy_flip[:, 0:1]\n",
    "        diff = fn(xy_q) + fn(xy_flip)\n",
    "        return (w_q*diff*diff).sum() / (w_q.sum() + 1e-12)\n",
    "\n",
    "def symmetry_penalty_even(fn, xy_q, w_q):\n",
    "    with torch.enable_grad():\n",
    "        xy_flip = xy_q.clone()\n",
    "        xy_flip[:, 0:1] = -xy_flip[:, 0:1]\n",
    "        diff = fn(xy_q) - fn(xy_flip)\n",
    "        return (w_q*diff*diff).sum() / (w_q.sum() + 1e-12)\n",
    "\n",
    "print('✓ Loss functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_collocation(X, Y, n, device):\n",
    "    xy = torch.empty((n, 2), device=device).uniform_(0.0, 1.0)\n",
    "    xy[:, 0] = (xy[:, 0] * 2.0 - 1.0) * X\n",
    "    xy[:, 1] = (xy[:, 1] * 2.0 - 1.0) * Y\n",
    "    return xy\n",
    "\n",
    "def make_quadrature_box(X, Y, n=128):\n",
    "    xs = torch.linspace(-X, X, n)\n",
    "    ys = torch.linspace(-Y, Y, n)\n",
    "    Xg, Yg = torch.meshgrid(xs, ys, indexing='xy')\n",
    "    xy = torch.stack([Xg.reshape(-1), Yg.reshape(-1)], dim=-1)\n",
    "    w = torch.full((xy.shape[0], 1), (2*X/(n-1)) * (2*Y/(n-1)))\n",
    "    return xy, w\n",
    "\n",
    "def compute_norm_value(model, xy_q, w_q):\n",
    "    with torch.no_grad():\n",
    "        psi = model(xy_q)\n",
    "        return torch.sum(w_q * psi * psi)\n",
    "\n",
    "def compute_density_features(xs_t, ys_t, psi_grid_t, a_param=None):\n",
    "    N = xs_t.numel()\n",
    "    P = psi_grid_t ** 2\n",
    "    dx = (xs_t[-1] - xs_t[0]) / (N - 1)\n",
    "    dy = (ys_t[-1] - ys_t[0]) / (N - 1)\n",
    "    cell = dx * dy\n",
    "    total_prob = float((P.sum() * cell).item())\n",
    "    Xg = xs_t.view(N, 1).expand(N, N)\n",
    "    Yg = ys_t.view(1, N).expand(N, N)\n",
    "    com_x = float(((P * Xg).sum() * cell / (total_prob + 1e-12)).item())\n",
    "    com_y = float(((P * Yg).sum() * cell / (total_prob + 1e-12)).item())\n",
    "    left_mask = (Xg < 0)\n",
    "    left_mass = float((P[left_mask].sum() * cell).item())\n",
    "    right_mass = float((P[~left_mask].sum() * cell).item())\n",
    "    P_flat = P.reshape(-1)\n",
    "    vals, idxs = torch.topk(P_flat, k=min(2, P_flat.numel()))\n",
    "    peaks = []\n",
    "    for v, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "        iy = idx % N\n",
    "        ix = idx // N\n",
    "        peaks.append({'x': float(xs_t[ix].item()), 'y': float(ys_t[iy].item()), 'p': float(v)})\n",
    "    return {\n",
    "        'total_prob': total_prob,\n",
    "        'com': {'x': com_x, 'y': com_y},\n",
    "        'side_mass': {'left': left_mass, 'right': right_mass},\n",
    "        'peaks': peaks\n",
    "    }\n",
    "\n",
    "print('✓ Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_state(model, p, xy_q, w_q, X, Y, epochs=3000, n_colloc=4096, lr=1e-3,\n",
    "                    lam_rr=1.0, lam_pde=1.0, lam_norm=1.0, device=None, lbfgs_iters=200,\n",
    "                    lam_ortho=0.0, ref_models=None, lam_sym_odd=0.0, lam_sym_even=0.0):\n",
    "    device = device or xy_q.device\n",
    "    vfun_batch = lambda xy: vectorized_biq_v(p, xy)\n",
    "    \n",
    "    # Freeze reference models\n",
    "    refs = ref_models or []\n",
    "    for r in refs:\n",
    "        r.eval()\n",
    "        for p_ref in r.parameters():\n",
    "            p_ref.requires_grad_(False)\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    history = {'E': [], 'Lres': [], 'Lnorm': [], 'NormValue': [], 'Overlap0': []}\n",
    "    best = {'E': float('inf'), 'state_dict': None}\n",
    "    \n",
    "    # Adam training\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        xy_c = sample_collocation(X, Y, n_colloc, device)\n",
    "        \n",
    "        E = rayleigh_ritz_energy(model, vfun_batch, xy_q, w_q)\n",
    "        Lres = pde_residual_loss(model, vfun_batch, xy_c)\n",
    "        Lnorm = normalization_penalty(model, xy_q, w_q)\n",
    "        Lsym = 0.0\n",
    "        if lam_sym_odd > 0.0:\n",
    "            Lsym += lam_sym_odd * symmetry_penalty_odd(model, xy_q, w_q)\n",
    "        if lam_sym_even > 0.0:\n",
    "            Lsym += lam_sym_even * symmetry_penalty_even(model, xy_q, w_q)\n",
    "        loss = lam_rr * E + lam_pde * Lres + lam_norm * Lnorm + Lsym\n",
    "        \n",
    "        # Orthogonality\n",
    "        overlap_val = None\n",
    "        if len(refs) > 0 and lam_ortho > 0.0:\n",
    "            with torch.no_grad():\n",
    "                prev_list = [r(xy_q) for r in refs]\n",
    "            psi_new = model(xy_q)\n",
    "            overlap_term = 0.0\n",
    "            overlap_vals = []\n",
    "            for psi_prev in prev_list:\n",
    "                osq = orthogonality_loss(psi_new, psi_prev, w_q)\n",
    "                overlap_term += osq\n",
    "                overlap_vals.append(torch.sqrt(osq + 1e-12))\n",
    "            loss += lam_ortho * overlap_term\n",
    "            if overlap_vals:\n",
    "                overlap_val = torch.max(torch.stack(overlap_vals))\n",
    "        \n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        opt.step()\n",
    "        \n",
    "        history['E'].append(float(E.detach().cpu()))\n",
    "        history['Lres'].append(float(Lres.detach().cpu()))\n",
    "        history['Lnorm'].append(float(Lnorm.detach().cpu()))\n",
    "        norm_val = compute_norm_value(model, xy_q, w_q)\n",
    "        history['NormValue'].append(float(norm_val.detach().cpu()))\n",
    "        if overlap_val is not None:\n",
    "            history['Overlap0'].append(float(overlap_val.detach().cpu()))\n",
    "        \n",
    "        if E.item() < best['E']:\n",
    "            best['E'] = float(E.item())\n",
    "            best['state_dict'] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        if ep % 500 == 0 or ep == 1:\n",
    "            msg = f'[ep {ep:5d}] E={E.item():.6f}  Lres={Lres.item():.3e}  Lnorm={Lnorm.item():.3e}  |psi|^2={norm_val.item():.5f}'\n",
    "            if overlap_val is not None:\n",
    "                msg += f'  overlap0={overlap_val.item():.3e}'\n",
    "            print(msg)\n",
    "    \n",
    "    # LBFGS refinement\n",
    "    if lbfgs_iters > 0:\n",
    "        print(f'Running LBFGS refinement ({lbfgs_iters} iters)...')\n",
    "        opt2 = torch.optim.LBFGS(model.parameters(), max_iter=lbfgs_iters, line_search_fn='strong_wolfe')\n",
    "        \n",
    "        def closure():\n",
    "            opt2.zero_grad(set_to_none=True)\n",
    "            xy_c2 = sample_collocation(X, Y, max(256, n_colloc // 4), device)\n",
    "            E2 = rayleigh_ritz_energy(model, vfun_batch, xy_q, w_q)\n",
    "            R2 = pde_residual_loss(model, vfun_batch, xy_c2)\n",
    "            N2 = normalization_penalty(model, xy_q, w_q)\n",
    "            Lsym2 = 0.0\n",
    "            if lam_sym_odd > 0.0:\n",
    "                Lsym2 += lam_sym_odd * symmetry_penalty_odd(model, xy_q, w_q)\n",
    "            if lam_sym_even > 0.0:\n",
    "                Lsym2 += lam_sym_even * symmetry_penalty_even(model, xy_q, w_q)\n",
    "            loss2 = lam_rr * E2 + lam_pde * R2 + lam_norm * N2 + Lsym2\n",
    "            if len(refs) > 0 and lam_ortho > 0.0:\n",
    "                with torch.no_grad():\n",
    "                    prev_list2 = [r(xy_q) for r in refs]\n",
    "                psi_new2 = model(xy_q)\n",
    "                overlap_term2 = 0.0\n",
    "                for psi_prev2 in prev_list2:\n",
    "                    osq2 = orthogonality_loss(psi_new2, psi_prev2, w_q)\n",
    "                    overlap_term2 += osq2\n",
    "                loss2 += lam_ortho * overlap_term2\n",
    "            loss2.backward()\n",
    "            return loss2\n",
    "        \n",
    "        opt2.step(closure)\n",
    "        \n",
    "        # Final evaluation\n",
    "        with torch.no_grad():\n",
    "            E2 = rayleigh_ritz_energy(model, vfun_batch, xy_q, w_q)\n",
    "            e2f = float(E2.detach().cpu())\n",
    "            if e2f < best['E']:\n",
    "                best['E'] = e2f\n",
    "                best['state_dict'] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        print(f'LBFGS done. Final E={best[\"E\"]:.6f}')\n",
    "    \n",
    "    # Load best\n",
    "    if best['state_dict'] is not None:\n",
    "        model.load_state_dict(best['state_dict'])\n",
    "    \n",
    "    return {'history': history, 'bestE': best['E']}\n",
    "\n",
    "print('✓ Training loop defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavefunction_density(xs, ys, psi_grid, title='Wavefunction Density', vmax=None):\n",
    "    density = psi_grid.cpu().numpy() ** 2\n",
    "    X, Y = np.meshgrid(xs.cpu().numpy(), ys.cpu().numpy(), indexing='ij')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.contourf(X, Y, density, levels=50, cmap='viridis', vmax=vmax)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.colorbar(im, ax=ax, label='|ψ|²')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_training_curves(history, title='Training History'):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    axes[0, 0].plot(history['E'])\n",
    "    axes[0, 0].set_ylabel('Energy')\n",
    "    axes[0, 0].set_title('Rayleigh-Ritz Energy')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].semilogy(history['Lres'])\n",
    "    axes[0, 1].set_ylabel('PDE Residual')\n",
    "    axes[0, 1].set_title('PDE Residual Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].semilogy(history['Lnorm'])\n",
    "    axes[1, 0].set_ylabel('Norm Penalty')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_title('Normalization Penalty')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(history['NormValue'])\n",
    "    axes[1, 1].axhline(1.0, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].set_ylabel('∫|ψ|² dΩ')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_title('Normalization Value')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    if 'Overlap0' in history and len(history['Overlap0']) > 0:\n",
    "        fig2, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.semilogy(history['Overlap0'])\n",
    "        ax.set_ylabel('Max Overlap')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_title('Orthogonality (Max Overlap to Refs)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print('✓ Visualization functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Material\n",
    "mat = GaAs()\n",
    "print(f'GaAs: L0={mat.L0_nm} nm, E0={mat.E0_meV:.3f} meV')\n",
    "\n",
    "# Potential parameters\n",
    "a = 1.5  # well separation\n",
    "hbar_omega_x = 3.0  # meV\n",
    "hbar_omega_y = 5.0  # meV\n",
    "delta = 0.0  # asymmetry\n",
    "p = from_targets(a, hbar_omega_x, hbar_omega_y, delta, mat)\n",
    "print(f'Potential: a={p.a}, c4={p.c4:.4f}, c2y={p.c2y:.4f}, δ={p.delta}')\n",
    "\n",
    "# Computational domain\n",
    "X, Y = 4.0, 4.0\n",
    "nq = 128  # quadrature grid\n",
    "nc = 8192  # collocation points\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "lbfgs_iters = 200\n",
    "\n",
    "# Loss weights\n",
    "lam_rr = 1.0\n",
    "lam_pde = 2.0\n",
    "lam_norm = 100.0\n",
    "lam_ortho = 20.0  # for excited states\n",
    "\n",
    "# Parity weights (for symmetric case δ=0)\n",
    "lam_sym_even = 0.1  # GS, ES2\n",
    "lam_sym_odd = 0.1   # ES1\n",
    "\n",
    "# Network architecture\n",
    "hidden_features = 128\n",
    "hidden_layers = 4\n",
    "\n",
    "# Create quadrature grid\n",
    "xy_q, w_q = make_quadrature_box(X, Y, nq)\n",
    "xy_q = xy_q.to(device)\n",
    "w_q = w_q.to(device)\n",
    "print(f'Quadrature grid: {xy_q.shape[0]} points')\n",
    "\n",
    "# Output directory\n",
    "outdir = Path('colab_results')\n",
    "outdir.mkdir(exist_ok=True)\n",
    "print(f'Results will be saved to: {outdir}')\n",
    "\n",
    "print('\\\n",
    "✓ Configuration complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Ground State (GS)\n",
    "\n",
    "Train the ground state with optional even-parity bias (for symmetric δ=0 case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING GROUND STATE (GS)')\n",
    "print('='*60)\n",
    "\n",
    "model_gs = SIREN(in_features=2, hidden_features=hidden_features, hidden_layers=hidden_layers).to(device)\n",
    "\n",
    "result_gs = train_one_state(\n",
    "    model_gs, p, xy_q, w_q, X, Y,\n",
    "    epochs=epochs,\n",
    "    n_colloc=nc,\n",
    "    lr=lr,\n",
    "    lam_rr=lam_rr,\n",
    "    lam_pde=lam_pde,\n",
    "    lam_norm=lam_norm,\n",
    "    device=device,\n",
    "    lbfgs_iters=lbfgs_iters,\n",
    "    lam_sym_even=lam_sym_even if delta == 0.0 else 0.0\n",
    ")\n",
    "\n",
    "E_gs = result_gs['bestE']\n",
    "print(f'\\\n",
    "✓ GS training complete. E_GS = {E_gs:.6f} (dimensionless)')\n",
    "print(f'  E_GS = {E_gs * mat.E0_meV:.4f} meV')\n",
    "\n",
    "# Save model\n",
    "torch.save(model_gs.state_dict(), outdir / 'model_gs.pt')\n",
    "with open(outdir / 'energies.txt', 'w') as f:\n",
    "    f.write(f'E_GS = {E_gs:.8f}  ({E_gs * mat.E0_meV:.6f} meV)\\\n",
    "')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Ground State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on grid\n",
    "N_plot = 200\n",
    "xs_plot = torch.linspace(-X, X, N_plot)\n",
    "ys_plot = torch.linspace(-Y, Y, N_plot)\n",
    "Xg_plot, Yg_plot = torch.meshgrid(xs_plot, ys_plot, indexing='xy')\n",
    "xy_plot = torch.stack([Xg_plot.reshape(-1), Yg_plot.reshape(-1)], dim=-1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    psi_gs_flat = model_gs(xy_plot).cpu()\n",
    "psi_gs_grid = psi_gs_flat.reshape(N_plot, N_plot)\n",
    "\n",
    "# Normalize for visualization\n",
    "dx_plot = (xs_plot[-1] - xs_plot[0]) / (N_plot - 1)\n",
    "dy_plot = (ys_plot[-1] - ys_plot[0]) / (N_plot - 1)\n",
    "norm_plot = (psi_gs_grid**2).sum() * dx_plot * dy_plot\n",
    "psi_gs_grid = psi_gs_grid / torch.sqrt(norm_plot + 1e-12)\n",
    "\n",
    "# Compute features\n",
    "features_gs = compute_density_features(xs_plot, ys_plot, psi_gs_grid, a_param=p.a)\n",
    "print('GS Density Features:')\n",
    "print(f'  COM: ({features_gs[\"com\"][\"x\"]:.4f}, {features_gs[\"com\"][\"y\"]:.4f})')\n",
    "print(f'  Side mass: L={features_gs[\"side_mass\"][\"left\"]:.4f}, R={features_gs[\"side_mass\"][\"right\"]:.4f}')\n",
    "print(f'  Top 2 peaks: {features_gs[\"peaks\"]}')\n",
    "\n",
    "# Plot\n",
    "fig_gs = plot_wavefunction_density(xs_plot, ys_plot, psi_gs_grid, title='Ground State |ψ_GS|²')\n",
    "plt.show()\n",
    "\n",
    "fig_hist_gs = plot_training_curves(result_gs['history'], title='GS Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train First Excited State (ES1)\n",
    "\n",
    "Train ES1 orthogonal to GS, with optional odd-parity bias (for symmetric δ=0 case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING FIRST EXCITED STATE (ES1)')\n",
    "print('='*60)\n",
    "\n",
    "model_es1 = SIREN(in_features=2, hidden_features=hidden_features, hidden_layers=hidden_layers).to(device)\n",
    "\n",
    "result_es1 = train_one_state(\n",
    "    model_es1, p, xy_q, w_q, X, Y,\n",
    "    epochs=epochs,\n",
    "    n_colloc=nc,\n",
    "    lr=lr,\n",
    "    lam_rr=lam_rr,\n",
    "    lam_pde=lam_pde,\n",
    "    lam_norm=lam_norm,\n",
    "    device=device,\n",
    "    lbfgs_iters=lbfgs_iters,\n",
    "    lam_ortho=lam_ortho,\n",
    "    ref_models=[model_gs],\n",
    "    lam_sym_odd=lam_sym_odd if delta == 0.0 else 0.0\n",
    ")\n",
    "\n",
    "E_es1 = result_es1['bestE']\n",
    "print(f'\\\n",
    "✓ ES1 training complete. E_ES1 = {E_es1:.6f} (dimensionless)')\n",
    "print(f'  E_ES1 = {E_es1 * mat.E0_meV:.4f} meV')\n",
    "print(f'  Splitting: ΔE = {(E_es1 - E_gs) * mat.E0_meV:.4f} meV')\n",
    "\n",
    "# Save\n",
    "torch.save(model_es1.state_dict(), outdir / 'model_es1.pt')\n",
    "with open(outdir / 'energies.txt', 'a') as f:\n",
    "    f.write(f'E_ES1 = {E_es1:.8f}  ({E_es1 * mat.E0_meV:.6f} meV)\\\n",
    "')\n",
    "    f.write(f'ΔE(ES1-GS) = {(E_es1-E_gs):.8f}  ({(E_es1-E_gs) * mat.E0_meV:.6f} meV)\\\n",
    "')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize First Excited State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    psi_es1_flat = model_es1(xy_plot).cpu()\n",
    "psi_es1_grid = psi_es1_flat.reshape(N_plot, N_plot)\n",
    "norm_plot_es1 = (psi_es1_grid**2).sum() * dx_plot * dy_plot\n",
    "psi_es1_grid = psi_es1_grid / torch.sqrt(norm_plot_es1 + 1e-12)\n",
    "\n",
    "features_es1 = compute_density_features(xs_plot, ys_plot, psi_es1_grid, a_param=p.a)\n",
    "print('ES1 Density Features:')\n",
    "print(f'  COM: ({features_es1[\"com\"][\"x\"]:.4f}, {features_es1[\"com\"][\"y\"]:.4f})')\n",
    "print(f'  Side mass: L={features_es1[\"side_mass\"][\"left\"]:.4f}, R={features_es1[\"side_mass\"][\"right\"]:.4f}')\n",
    "print(f'  Top 2 peaks: {features_es1[\"peaks\"]}')\n",
    "\n",
    "fig_es1 = plot_wavefunction_density(xs_plot, ys_plot, psi_es1_grid, title='First Excited State |ψ_ES1|²')\n",
    "plt.show()\n",
    "\n",
    "fig_hist_es1 = plot_training_curves(result_es1['history'], title='ES1 Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Second Excited State (ES2)\n",
    "\n",
    "Train ES2 orthogonal to both GS and ES1, with optional even-parity bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING SECOND EXCITED STATE (ES2)')\n",
    "print('='*60)\n",
    "\n",
    "model_es2 = SIREN(in_features=2, hidden_features=hidden_features, hidden_layers=hidden_layers).to(device)\n",
    "\n",
    "result_es2 = train_one_state(\n",
    "    model_es2, p, xy_q, w_q, X, Y,\n",
    "    epochs=epochs,\n",
    "    n_colloc=nc,\n",
    "    lr=lr,\n",
    "    lam_rr=lam_rr,\n",
    "    lam_pde=lam_pde,\n",
    "    lam_norm=lam_norm,\n",
    "    device=device,\n",
    "    lbfgs_iters=lbfgs_iters,\n",
    "    lam_ortho=lam_ortho,\n",
    "    ref_models=[model_gs, model_es1],\n",
    "    lam_sym_even=lam_sym_even if delta == 0.0 else 0.0\n",
    ")\n",
    "\n",
    "E_es2 = result_es2['bestE']\n",
    "print(f'\\\n",
    "✓ ES2 training complete. E_ES2 = {E_es2:.6f} (dimensionless)')\n",
    "print(f'  E_ES2 = {E_es2 * mat.E0_meV:.4f} meV')\n",
    "print(f'  Splitting: ΔE(ES2-ES1) = {(E_es2 - E_es1) * mat.E0_meV:.4f} meV')\n",
    "\n",
    "# Save\n",
    "torch.save(model_es2.state_dict(), outdir / 'model_es2.pt')\n",
    "with open(outdir / 'energies.txt', 'a') as f:\n",
    "    f.write(f'E_ES2 = {E_es2:.8f}  ({E_es2 * mat.E0_meV:.6f} meV)\\\n",
    "')\n",
    "    f.write(f'ΔE(ES2-ES1) = {(E_es2-E_es1):.8f}  ({(E_es2-E_es1) * mat.E0_meV:.6f} meV)\\\n",
    "')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Second Excited State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    psi_es2_flat = model_es2(xy_plot).cpu()\n",
    "psi_es2_grid = psi_es2_flat.reshape(N_plot, N_plot)\n",
    "norm_plot_es2 = (psi_es2_grid**2).sum() * dx_plot * dy_plot\n",
    "psi_es2_grid = psi_es2_grid / torch.sqrt(norm_plot_es2 + 1e-12)\n",
    "\n",
    "features_es2 = compute_density_features(xs_plot, ys_plot, psi_es2_grid, a_param=p.a)\n",
    "print('ES2 Density Features:')\n",
    "print(f'  COM: ({features_es2[\"com\"][\"x\"]:.4f}, {features_es2[\"com\"][\"y\"]:.4f})')\n",
    "print(f'  Side mass: L={features_es2[\"side_mass\"][\"left\"]:.4f}, R={features_es2[\"side_mass\"][\"right\"]:.4f}')\n",
    "print(f'  Top 2 peaks: {features_es2[\"peaks\"]}')\n",
    "\n",
    "fig_es2 = plot_wavefunction_density(xs_plot, ys_plot, psi_es2_grid, title='Second Excited State |ψ_ES2|²')\n",
    "plt.show()\n",
    "\n",
    "fig_hist_es2 = plot_training_curves(result_es2['history'], title='ES2 Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Material: GaAs (L0={mat.L0_nm} nm, E0={mat.E0_meV:.3f} meV)')\n",
    "print(f'Potential: a={p.a}, ħωx={hbar_omega_x} meV, ħωy={hbar_omega_y} meV, δ={delta}')\n",
    "print()\n",
    "print('Energies (dimensionless):')\n",
    "print(f'  E_GS  = {E_gs:.8f}')\n",
    "print(f'  E_ES1 = {E_es1:.8f}  (ΔE = {E_es1-E_gs:.8f})')\n",
    "print(f'  E_ES2 = {E_es2:.8f}  (ΔE = {E_es2-E_es1:.8f})')\n",
    "print()\n",
    "print('Energies (meV):')\n",
    "print(f'  E_GS  = {E_gs * mat.E0_meV:.6f} meV')\n",
    "print(f'  E_ES1 = {E_es1 * mat.E0_meV:.6f} meV  (ΔE = {(E_es1-E_gs) * mat.E0_meV:.6f} meV)')\n",
    "print(f'  E_ES2 = {E_es2 * mat.E0_meV:.6f} meV  (ΔE = {(E_es2-E_es1) * mat.E0_meV:.6f} meV)')\n",
    "print()\n",
    "print(f'Results saved to: {outdir}')\n",
    "print('='*60)\n",
    "\n",
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, psi_grid, title in zip(axes, [psi_gs_grid, psi_es1_grid, psi_es2_grid], ['GS', 'ES1', 'ES2']):\n",
    "    density = psi_grid.cpu().numpy() ** 2\n",
    "    X_mesh, Y_mesh = np.meshgrid(xs_plot.cpu().numpy(), ys_plot.cpu().numpy(), indexing='ij')\n",
    "    im = ax.contourf(X_mesh, Y_mesh, density, levels=50, cmap='viridis')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{title} |ψ|²')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(outdir / 'all_states.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\\n",
    "✓ All done! Check the colab_results/ directory for saved models and plots.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
